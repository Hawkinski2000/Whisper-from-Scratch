# Whisper-from-Scratch
This project is a reimplementation of OpenAI's voice transcription model, [Whisper](https://openai.com/index/whisper/), from scratch, using a modified version of [Andrej Karpathy's reproduction of GPT-2 (124M)](https://github.com/karpathy/build-nanogpt 'build-nanogpt'). Some components of the Whisper model code are directly used from the [official Whisper repository by OpenAI](https://github.com/openai/whisper/tree/main).

This project is an independent reimplementation and is not affiliated with or endorsed by OpenAI.

* Added an Encoder and Cross-Attention to GPT-2 for translation of audio spectrograms to text.
* Trained on the LibriSpeech ASR Corpus on an 8x A100 (80 GB SXM4) instance from Lambda Labs.
* Currently training on Common Voice Corpus 17.0

The following training loss curve was generated from training for 1000 steps on the LibriSpeech ASR Corpus:

![training_loss_curve](training_loss_curve.png)

Audio samples were converted to Log-mel spectrograms like the one below:

![mel_spectrogram](mel_spectrogram.png)

These spectrograms are fed through 2x Conv1D + GELU layers before entering the Transformer's encoder blocks. The corresponding transcriptions are tokenized using the GPT-2 tokenizer, which enter the Transformer's decoder blocks. The embeddings generated by the encoder interact with those generated from self-attention in the decoder through cross-attention. This allows the model to learn how to associate audio features with corresponding text features. Below is a visualization of the original Transformer model from the paper "Attention Is All You Need":

![transformer](transformer.png)

Image credit: https://arxiv.org/abs/1706.03762
